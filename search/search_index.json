{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Workshop on Architecture Support for Embodied AI Systems Workshop on Architecture Support for Embodied AI Systems <p>This workshop is in conjunction with ISCA 2025</p> <p>Please register for ISCA to get Zoom access to this event</p> <ul> <li>\ud83d\udd61 Duration: Half Day</li> <li>\ud83d\udcc5 Date: Saturday, June 21, 2025</li> <li>\u23f0 Start Time: 1:00 PM JST</li> <li>\ud83c\udfab Registration: Sign Up Here</li> <li>\ud83d\udcc6 Paper Submission Deadline: Tuesday Apr 29, 2025, 7:59:59 AM UTC</li> <li>\ud83d\udece\ufe0f Notification of Acceptance: May 5, 2025</li> </ul> Introduction <p>Robots are becoming increasingly intelligent, evolving into embodied AI systems powered by advanced machine learning models\u2014and that's exactly what we want. However, the computing stack that supports these embodied AI robots is lagging behind. This workshop aims to take the first step in bridging the gap between the computer architecture and robotics communities.</p> <p>The goal of building an efficient computing stack for embodied AI robots echoes what our field has achieved over the past decades: faster, more energy-efficient chips. Yet the challenges here are fundamentally different. Workloads are evolving rapidly, model inference demands are novel, real-time constraints are stringent, safety and reliability requirements are critical, and robots run out of power fast. This emerging domain presents a host of new and exciting challenges. Through this workshop, we aim to highlight these topics and bring them to the attention of the computer architecture community.</p> <p>The program will feature a mix of invited talks and contributed posters. We welcome submissions from all related areas\u2014please refer to the CFP below for more details.</p> Call For Papers Topics of Interest <p>In general, we welcome system designers and researchers on <code>robotic computing</code> to join this workshop and present their ongoing works. The topics we are interested in include but are not limited to:</p> 1. Innovative Computer Architecture Design for Robotic Computing <ul> <li>Accelerators for localization, navigation, path planning, and robotic control algorithms</li> <li>Accelerators for vision-language-action and vision-language-model inference on edge devices</li> <li>Accelerators for LLM inference on edge devices</li> <li>Accelerating robotic computing algorithms with general-purpose processors</li> </ul> 2. Detailed Benchmarking for Embodied AI Systems <ul> <li>Benchmarking for localization, navigation, path planning, and robotic control algorithms</li> <li>Benchmarking for vision-language-action and vision-language models</li> <li>Benchmarking for real embodied AI workloads</li> <li>Advanced chip design methodology</li> </ul> 3. Cost-Driven Chip Design <ul> <li>Design-and-technology co-optimization</li> <li>Agile chip design (HLS and AI for EDA)</li> <li>Multi-chiplet design methodology</li> </ul> Submission Guidelines <ul> <li>Arc4EAI welcomes submissions of short papers, limited to 2 pages (excluding references), formatted in a double-column layout. Authors are encouraged to use the provided LaTeX template.</li> <li>Submissions must clearly define the research problem, its motivation, and technical contributions, written in English and submitted as a single PDF file.</li> <li>Papers may include works in progress, exploratory or preliminary studies, or previously published research.</li> <li>Submissions will be evaluated based on novelty, technical quality, potential impact, community interest, clarity, relevance to the workshop, and reproducibility.</li> <li>Reviews will be non-blind; authors must include their names and affiliations in the PDF without anonymization.</li> <li>Accepted papers will not appear in formal proceedings but will be published on the workshop website, allowing authors to further develop and submit their work elsewhere.</li> <li>At least one author of each accepted paper must attend the workshop and present their work.</li> </ul> Paper Submission System <p>https://arch4eai25.hotcrp.com/</p> Organizing Chairs and Bios <ul> <li>Dr. Shaoshan Liu</li> <li>Dr. Yiming Gan</li> <li>Dr. Xing Hu</li> </ul> Organizing Committee <ul> <li>Web Chair: Wenhao SunEmail: sunwenhao23@mails.ucas.ac.cn</li> <li>Publicity Chair: Yuhui HaoEmail: yuhuihao@tju.edu.cn</li> <li>Registration Chair: Mengdi WangEmail: wangmengdi@ict.ac.cn</li> </ul> Online Participation <p>We are planning to support online participation. All the invited presenters will share their slides and talks via Zoom or other online meeting software.</p> Invited Speakers Time Name Title Affiliation Talk Topic 13:00 - 13:45 Vijay Janapa Reddi Associate Professor Harvard University Benchmarking Robotic Applications 13:45 - 14:30 Jean-Luc Gaudiot Distinguished Professor UC Irvine Dataflow Accelerator Architecture for Autonomous Machine Computing 14:30 - 14:40 Coffee Break 14:40 - 15:25 OGATA Tetsuya Professor Waseda University Cognitive Developmental Robotics 15:25 - 16:10 Zhenhua Zhu Postdoc Tsinghua University &amp; HKUST LLM Inference at Edge Speaker Details Vijay Janapa Reddi Talk Title <p>Embodied AI in the Wild: System Bottlenecks and Building Trustworthy Embodied AI</p> Abstract <p>Embodied AI systems, where generative agents integrate perception, cognition, action, and reasoning through large language models (LLMs), hold tremendous promise for tackling complex, long-horizon tasks in dynamic, real-world environments. However, deploying these systems in the physical world introduces a tightly coupled set of challenges that span performance, scalability, system design, and safety. This talk presents a holistic workload analysis that examines how architectural choices, coordination strategies, and task demands interact to shape overall system behavior. A taxonomy of representative paradigms and a benchmark suite provide the basis for systematic evaluation across agent configurations and task types. The analysis reveals critical bottlenecks, including planning and communication delays, memory inconsistencies, and declining coordination efficiency, which emerge as consequences of trade-offs made across the software and hardware stack. These findings motivate cross-layer co-design strategies that address performance, scalability, and robustness as interdependent concerns. At the same time, the integration of generative models into physical systems raises pressing safety questions. To support responsible deployment, safety scorecards are introduced as a framework for identifying and communicating system-level risks. By linking workload characteristics, design trade-offs, and safety implications, the talk offers a unified perspective on building the next generation of embodied intelligence systems that are efficient, scalable, and trustworthy by design.</p> Biography <p>Dr. Vijay Janapa Reddi is an Associate Professor of Engineering and Applied Sciences at Harvard University, where his research focuses on the intersection of computer architecture, machine learning systems, and autonomous agents. His multidisciplinary expertise drives advancements in efficient and intelligent computing systems across scales, from mobile and edge platforms to Internet of Things (IoT) devices. Prior to joining Harvard, Dr. Janapa Reddi was an Associate Professor in the Department of Electrical and Computer Engineering at the University of Texas at Austin. In addition to his academic role, Dr. Janapa Reddi is deeply involved in shaping the future of machine learning and edge AI technologies. He serves as Vice President and co-founder of MLCommons, a nonprofit organization dedicated to accelerating machine learning innovation. In this capacity, he oversees the MLCommons Research organization, sits on its board of directors, and co-led the development of the MLPerf benchmarks, which evaluates a wide range of ML systems from megawatt to microwatt scales. Dr. Janapa Reddi also serves on the boards of directors for the EDGE AI Foundation, fostering academic-industry partnerships at the edge of AI. Throughout his career, Dr. Janapa Reddi has earned numerous awards and accolades, including the Gilbreth Lecturer Honor from the National Academy of Engineering (NAE) in 2016, the IEEE TCCA Young Computer Architect Award (2016), the Intel Early Career Award (2013), and Google Faculty Research Awards in 2012, 2013, 2015, 2017, and 2020. He has also received Best Paper awards at the 2020 Design Automation Conference (DAC), the 2005 International Symposium on Microarchitecture (MICRO), and the 2009 International Symposium on High-Performance Computer Architecture (HPCA). Additionally, he has won various honors and awards, including IEEE Top Picks in Computer Architecture (2006, 2010, 2011, 2016, 2017, 2022, 2023). He is included in the MICRO and HPCA Halls of Fame (inducted in 2018 and 2019, respectively). Dr. Janapa Reddi is passionate about expanding access to applied machine learning and promoting diversity in STEM. He has developed an open-source book, \"Machine Learning Systems,\" (mlsysbook.ai) which is widely adopted by institutions worldwide. Additionally, he created the Tiny Machine Learning (TinyML) series on edX, a massive open online course that has trained over 100,000 students globally in recent years. Dr. Janapa Reddi holds a Ph.D. in computer science from Harvard University, an M.S. in electrical and computer engineering from the University of Colorado at Boulder, and a B.S. in computer engineering from Santa Clara University. </p> Jean-Luc Gaudiot Talk Title <p>Dataflow Accelerator Architecture for Autonomous Machine Computing</p> Abstract <p>Commercial autonomous machines is a thriving sector, one that is likely the next ubiquitous computing platform, after Personal Computers (PC), cloud computing, and mobile computing. Nevertheless, a suitable computing substrate for autonomous machines is missing, and many companies are forced to develop ad hoc computing solutions that are neither principled nor extensible. By analyzing the demands of autonomous machine computing, this talk introduces Dataflow Accelerator Architecture (DAA), a modern instantiation of the classic dataflow principle, that matches the characteristics of autonomous machine software, such as embodied AI systems.</p> Biography <p>Professor Jean-Luc Gaudiot is currently Distinguished Professor in the Electrical Engineering and Computer Science Department at the University of California, Irvine where he was department Chair from 2003 to 2009. During his tenure, the department underwent significant changes. These include the hiring of twelve new faculty members (three senior professors) and the remarkable rise in the US News and World Report rankings of both the Computer Engineering program and the Electrical Engineering program. From 1999 to 2002, he was the Editor-in-Chief of the IEEE Transactions on Computers. He served as the 2017 IEEE Computer Society President. He is a Fellow of IEEE and AAAS.</p> OGATA Tetsuya Talk Title <p>Open Ecosystem for Foundational Robot Models</p> Abstract <p>In recent years, a race to develop foundational robot models capable of End-to-End robot control has been particularly active, especially in the United States and China. These foundational models are expected not only to accelerate robot development but also to significantly expand their application areas. However, while the models themselves are publicly available, some of the utilization data and development methodologies remain undisclosed, which can be considered a limitation for their practical application. In this presentation, I will introduce the ecosystem for open foundational robot model development within the AI Robot Association, where I serve as Chairman, and discuss future prospects.</p> Biography <p>OGATA Tetsuya received the B.S., M.S., and D.E. degrees in mechanical engineering from Waseda University, Tokyo, Japan, in 1993, 1995, and 2000, respectively. He was a Research Associate with Waseda University from 1999 to 2001. From 2001 to 2003, he was a Research Scientist with the RIKEN Brain Science Institute, Saitama, Japan. From 2003 to 2012, he was an Associate Professor at the Graduate School of Informatics, Kyoto University, Kyoto, Japan. Since 2012, he has been a Professor with the Faculty of Science and Engineering, at Waseda University. Since 2017, he is a Joint-appointed Fellow with the Artificial Intelligence Research Center, National Institute of Advanced Industrial Science and Technology (AIST), Tokyo. Since 2024, he is a visiting professor at the Research and Development Center for Large Language Models, National Institute of Informatics (NII), Tokyo.</p> Zhenhua Zhu Talk Title <p>Towards Energy Efficient Architecture and System Design for AI 2.0 Era</p> Abstract <p>Transformer-based Large Models have achieved outstanding performance across various applications including embodied AI and intelligent agents, marking the advent of the AI 2.0 era. With the exponential increase in model parameters, the computational, storage, and memory access overhead of large models have increased by four to five orders of magnitude compared to traditional deep learning models, resulting in significant differences between the algorithm requirements and capabilities of existing hardware platforms. This talk will introduce energy-efficient architecture and system design methodology for AI 2.0. Focusing on various AIGC algorithms like LLM and diffusion model, this talk will discuss software and hardware co-optimization methods for GPU, FPGA, and emerging Processing-In-Memory architectures. Our optimization methods can reduce the total cost of large model inference by four orders of magnitude. Finally, we will provide an outlook on future trends of embodied agents.</p> Biography <p>Zhenhua Zhu received his B.S. and Ph.D. (with honor) degrees from the Department of Electronic Engineering, Tsinghua University, China, in 2018 and 2024, respectively. Currently, he is serving as a postdoc researcher in the Department of EE, Tsinghua University and a visiting scholar in the Department of ECE, HKUST. His research interests include Processing-In-Memory, Near Memory Computing, computer architecture, EDA, etc. He has published/accepted 50 academic papers in IEEE TCAD, DAC, ISCA, MICRO, ICCAD, ASPLOS, and DATE, with Google Scholar citations more than 1,400. He has received Best Paper Honorable Mention in HPCA 2025, Best Paper Nomination in DATE 2023. He serves as a reviewer for IEEE TCAD, IEEE TVLSI, ACM TODAES, ACM TECS, CASES, GLSVLSI, ISVLSI, and AICAS. He also serves as the TPC secretary of ASP-DAC 2025.</p> Poster and Light Talk <p>Note: Each presenter should prepare a 2-minute slides and an A0 poster.</p> Time Name Title 16:10 - 16:12 Tong Zhang Hardware-Algorithm Co-Design for Robotic Edge Intelligence: CIM-Optimized CNN Architectures with Adaptive Kernel Scaling 16:12 - 16:14 Shunchen Shi Mixed-Precision Processing-in-Memory Architecture for Edge Device LLM Inference 16:14 - 16:16 Wenhao Sun Investigating Performance and Real-Time Trade-offs in Out-of-Order Processors 16:16 - 16:18 Jieke Lin KAITIAN: Communication for Heterogeneous Accelerators in Embodied AI 16:18 - 16:20 Haining Tan Co-Design of Vision Transformers and Accelerators for Efficient Inference on Spaceborne Edge Devices 16:20 - 16:30 Q&amp;A Session Q&amp;A Session Time Activity 16:20 - 16:30 Open Discussion and Q&amp;A Contact Us <p>Any questions may be directed to: ganyiming@ict.ac.cn</p>"}]}